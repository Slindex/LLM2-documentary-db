# LLM2-db-documental

# Streamlit Chat con Langchain 

Este es un proyecto que utiliza Streamlit y Langchain para crear un chat basado en modelos de lenguaje de Hugging Face. Con esta aplicaci칩n, se puede interactuar con un modelo de lenguaje que obtiene la informaci칩n de los documentos cargados por el usuario y responde a las preguntas basado en dicha informaci칩n

## Requisitos

El primer paso es acceder al directorio de trabajo y crear el entorno virtual. Luego acceder al mismo

```bash
cd ruta/al/directorio
python -m venv st_venv
.\st_venv\Scripts\activate

```

Una vez en el entorno virtual se deben instalar las bibliotecas del archivo requirements.txt:

```bash
langchain
torch
accelerate
sentence_transformers
streamlit_chat
streamlit
faiss-cpu
tiktoken
ctransformers
huggingface-hub
pypdf
python-dotenv
replicate
docx2txt
```

## Uso

Se ejecuta la aplicaci칩n en streamlit con el siguiente comando:
```bash
streamlit run app.py
```

## C칩digo

El archivo app.py, importa las siguientes librer칤as:
```bash
import streamlit as st
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.text_splitter import CharacterTextSplitter
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader
from langchain.document_loaders import TextLoader
from langchain.document_loaders import Docx2txtLoader
import os
import tempfile
import utils as utils
import importlib
importlib.reload(utils)
```

Se define una funci칩n llamada main() que ser치 el punto de entrada principal de la aplicaci칩n de chatbot en Streamlit.
```bash
def main():
```

Se llama a la funci칩n initializeSessionState() del m칩dulo utils para inicializar el estado de la sesi칩n de la aplicaci칩n.
```bash
utils.initializeSessionState()
```

Esta funci칩n se encarga de configurar el estado de la sesi칩n. Verifica si ciertas claves ('history', 'generated', 'past') est치n presentes en el estado de la sesi칩n.
Si no est치n presentes, las inicializa con valores predeterminados.
```bash
def initializeSessionState():

    if 'history' not in st.session_state:
        st.session_state['history'] = []

    if 'generated' not in st.session_state:
        st.session_state['generated'] = ["Hola! Preguntame lo que quieras sobre los documentos 游뱅"]

    if 'past' not in st.session_state:
        st.session_state['past'] = ["Hola! 游녦"]
```

Se establece el t칤tulo principal de la aplicaci칩n y se crea una barra lateral en la interfaz de usuario donde los usuarios pueden cargar m칰tliples documentos.
```bash
st.title("ChatBot de documentos usando llama2 :books:")
st.sidebar.title("Procesamiento de documentos")
    uploadedFiles = st.sidebar.file_uploader("Suba aqu칤 los documentos", accept_multiple_files=True)
```

Si se han cargado archivos, se inicializa una lista vac칤a llamada 'text'.
```bash
    if uploadedFiles:
        text = []
```

Se recorren uno a uno los archivos cargados y se obtiene la extensi칩n del nombre de cada uno.
```bash
        for file in uploadedFiles:
            fileExtension = os.path.splitext(file.name)[1]
```
Se utiliza la biblioteca tempfile.NamedTemporaryFile() para almacenar temporalmente datos en disco. El par치metro delete=False asegura que el archivo temporal no se elimine autom치ticamente cuando se cierre.
El contenido del archivo cargado (file) se escribe en el archivo temporal (tempFile) utilizando el m칠todo write().
Luego se obtiene la ruta del archivo temporal reci칠n creado y se almacena en la variable tempFilePath
```bash
            with tempfile.NamedTemporaryFile(delete=False) as tempFile:
                tempFile.write(file.read())
                tempFilePath = tempFile.name
```

Se crea la variable loader que contendr치 un objeto de la clase PyPDFLoader, Docx2txtLoader o TextLoader dependiendo la extensi칩n del archivo 'file'.
El objeto generado ser치 un documento que tendr치: el contenido del archivo como una cadena de texto y la metadata 
```bash
            loader = None
            if fileExtension == ".pdf":
                loader = PyPDFLoader(tempFilePath)
            elif fileExtension == ".docx" or fileExtension == ".doc":
                loader = Docx2txtLoader(tempFilePath)
            elif fileExtension == ".txt":
                loader = TextLoader(tempFilePath)
```
Si loader contiene un objeto, se carga el contenido de ese objeto (texto y metadatos) a la variable text y se borra el archivo temporal. De este modo la variable text contendr치 una lista de documentos con su texto y su metadata (etiquetas, nombres de archivo). El bucle 'for' vuelve a recorrer uploadedFiles hasta realizar el proceso con todos los archivos cargados.
 ```bash
            if loader:
                text.extend(loader.load())
                os.remove(tempFilePath)
```


Se instancia un objeto de la clase CharacterTextSplitter, con los siguientes argumentos:
* separator: En este caso, estamos usando "\n" como separador, lo que significa que cada fragmento ser치 separado por dos saltos de l칤nea consecutivos.
* chunk_size: el tama침o m치ximo deseado para cada fragmento. En este caso, se establece el tama침o m치ximo en 1000 caracteres.
* chunk_overlap: el n칰mero de caracteres que se solapan entre dos fragmentos adyacentes. En este caso, se establece el overlap en 100 caracteres.
* length_function: la funci칩n que se utilizar치 para medir la longitud de cada fragmento. En este caso, se utiliza la funci칩n integrada len() para contar el n칰mero de caracteres en cada fragmento.

Luego split_documents() recibe la cadena de texto original y la divide en fragmentos seg칰n los par치metros especificados en la instancia de CharacterTextSplitter. El resultado es una lista de fragmentos de texto, donde cada fragmento tiene un tama침o menor o igual al valor especificado en chunk_size. Si un fragmento supera el tama침o m치ximo, se dividen en varios fragmentos menores y se agregan a la lista resultante.
 ```bash
        textSplitter = CharacterTextSplitter(separator="\n", chunk_size=1000, chunk_overlap=100, length_function=len)
        textChunks = textSplitter.split_documents(text)
```

Se carga el modelo capaz de generar representaciones num칠ricas de texto, llamadas "embeddings", y se establece que debe ser ejecutado en una tarjeta gr치fica (device='cuda') para realizar las operaciones matem치ticas necesarias

 ```bash
        embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2", 
                                           model_kwargs={'device': 'cuda'})
```

Se utiliza la clase FAISS de la biblioteca langchain.vectorstores.faiss para crear un objeto VectorStore. La funci칩n from_documents() toma dos argumentos: una lista de objetos Document y un objeto Embeddings. En este caso, se est치n pasando los siguientes valores como argumentos:
textChunks: Es una lista de cadenas de caracteres que representan las diferentes partes del texto que se desea indexar. Cada cadena corresponde a un fragmento del texto original.
embeddings: Es un objeto Embeddings que contiene informaci칩n sobre c칩mo codificar el texto en vectores num칠ricos
 ```bash
        vectorStore = FAISS.from_documents(textChunks, embedding=embeddings)
```

Se pasa el objeto vectorStore a la funci칩n createConversationalChain, y se asigna a la variable 'chain'
```bash
        chain = utils.createConversationalChain(vectorStore)```
```
La funci칩n createConversationalChain crea una cadena de conversaciones para generar respuestas basadas en el contexto del chat. Utiliza un modelo de lenguaje (LLM) para generar texto y un recuperador para responder seg칰n el contexto. Adem치s, utiliza una memoria para almacenar las conversaciones previas y ayudar a mejorar la comprensi칩n del contexto.

La funci칩n comienza inicializando una cadena de conversaciones llamando a la clase ConversationalRetrievalChain. Esta clase es parte de la biblioteca de Python transformers, utilizada para implementar modelos de lenguaje y procesamiento de lenguaje natural.

Luego, la funci칩n configura los par치metros espec칤ficos para la generaci칩n de texto, la b칰squeda y la gesti칩n de memoria. Estos par치metros incluyen la temperatura del modelo de lenguaje, el tipo de cadena de conversaciones, el n칰mero m치ximo de nuevas tokens permitidas, si se debe realizar muestreo, el n칰mero de secuencias de regresi칩n deseadas y el identificador del token de fin de oraci칩n.

Finalmente, la funci칩n devuelve la cadena de conversaciones configurada.
```bash
def createConversationalChain(vectorStore):
    pipe = pipeline("text-generation",
                model=model,
                tokenizer= tokenizer,
                torch_dtype=torch.bfloat16,
                device_map="auto",
                max_new_tokens = 512,
                do_sample=True,
                top_k=30,
                num_return_sequences=1,
                eos_token_id=tokenizer.eos_token_id
                )
    llm = HuggingFacePipeline(pipeline = pipe, model_kwargs = {'temperature':0.01})
   
    memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)

    chain = ConversationalRetrievalChain.from_llm(llm=llm, chain_type='stuff',
                                                 retriever=vectorStore.as_retriever(search_kwargs={"k": 2}),
                                                 memory=memory)
    return chain
```

Por 칰ltimo, se llama a la funci칩n displayChatHistory, pas치ndole de argumento la cadena de conversaciones generada anteriormente
```bash
        utils.displayChatHistory(chain)
```

La funci칩n displayChatHistory muestra una interfaz de chat para interactuar con una cadena de conversaci칩n y ver el historial de chat, el cual incluye las consultas de los usuarios y las respuestas del sistema en un formato de conversaci칩n.
Argumentos:
chain (callable): Una funci칩n u objeto callable que toma un diccionario con una pregunta
y chat_history como entrada, y devuelve un diccionario con una respuesta.
```bash
def displayChatHistory(chain):
    replyContainer = st.container()
    container = st.container()

    with container:
        with st.form(key='my_form', clear_on_submit=True):
            userInput = st.text_input("Pegunta:", placeholder="Pregunta sobre los documentos", key='input')
            submitButton = st.form_submit_button(label='Enviar')

        if submitButton and userInput:
            with st.spinner('Generando respuesta...'):
                output = conversationChat(userInput, chain, st.session_state['history'])

            st.session_state['past'].append(userInput)
            st.session_state['generated'].append(output)

    if st.session_state['generated']:
        with replyContainer:
            for i in range(len(st.session_state['generated'])):
                message(st.session_state["past"][i], is_user=True, key=str(i) + '_user', avatar_style="thumbs")
                message(st.session_state["generated"][i], key=str(i), avatar_style="fun-emoji")
```
La funci칩n displayChatHistory utiliza por dentro a la funci칩n conversationChat, la cual tiene varios componentes importantes:

result = chain({"question": query, "chat_history": history}): Aqu칤 se llama a la funci칩n chain con un diccionario que contiene la pregunta actual (query) y el historial de conversaci칩n anterior (history). La funci칩n chain devuelve un resultado que incluye la respuesta a la pregunta actual.

history.append((query, result["answer"])): Se agrega la pregunta actual y su respuesta correspondiente al final del historial de conversaci칩n.

return result["answer"]: Finalmente, se devuelve solo la respuesta a la pregunta actual.

En otras palabras, esta funci칩n simplemente env칤a una pregunta a la funci칩n chain y obtiene una respuesta. Luego, agrega la pregunta y la respuesta al historial de conversaci칩n y devuelve la respuesta.
```bash
def conversationChat(query, chain, history):
    result = chain({"question": query, "chat_history": history})
    history.append((query, result["answer"]))
    return result["answer"]
```



***
***
## Google Colab

游뚾 En construcci칩n 游뚾

(Pr칩ximamente las instrucciones para ejecutar el modelo de chatbot desde Google Colab)


## Colaboradores

- Carla Celina Pezzone
- David Echajaya
- Jerem칤as Pombo
- Jerson Carbajal Ramirez
- Ren칠 Sebasti치n Joo Cisneros